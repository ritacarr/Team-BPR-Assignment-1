{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "N_TRAJECTORIES = 5000\n",
    "TRAJECTORY_LENGTH = 257\n",
    "VALIDATION_SPLIT = 0.2  # fraction of the training data used for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train shape: (1285000, 14)\n",
    "# 1285000 / 257 = 5000 trajectories\n",
    "data = pd.read_csv(\"X_train.csv\")\n",
    "n_lines = data.shape[0]\n",
    "assert n_lines / TRAJECTORY_LENGTH == N_TRAJECTORIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array([0, 1, ..., 4999]) with IDs of all trajectories\n",
    "all_trajectories = np.arange(N_TRAJECTORIES)\n",
    "# number of trajectories (e.g., 1000) used for validation\n",
    "n_trajectories_val = int(N_TRAJECTORIES * 0.2)\n",
    "# indices (from 0 to 4999) that identify the validation trajectories\n",
    "trajectories_val = np.random.choice(N_TRAJECTORIES, n_trajectories_val, replace=False)\n",
    "# the train trajectories IDs are those remaining from removing the validation trajectories from all 5000\n",
    "trajectories_train = np.setdiff1d(all_trajectories, trajectories_val)\n",
    "n_trajectories_train = N_TRAJECTORIES - n_trajectories_val\n",
    "\n",
    "# now we find the indices of the rows corresponding to each set (train and validation) of trajectories\n",
    "validation_indices = trajectories_val.repeat(TRAJECTORY_LENGTH - 1) * (TRAJECTORY_LENGTH - 1)\n",
    "validation_indices += np.tile(np.arange(TRAJECTORY_LENGTH - 1), n_trajectories_val)\n",
    "train_indices = trajectories_train.repeat(TRAJECTORY_LENGTH - 1) * (TRAJECTORY_LENGTH - 1)\n",
    "train_indices += np.tile(np.arange(TRAJECTORY_LENGTH - 1), n_trajectories_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[all_trajectories * TRAJECTORY_LENGTH][[\"x_1\", \"y_1\", \"x_2\", \"y_2\", \"x_3\", \"y_3\"]]\n",
    "X = X.to_numpy().repeat(TRAJECTORY_LENGTH - 1, axis=0)\n",
    "X = np.concatenate((X, data[data.index % TRAJECTORY_LENGTH != 0][[\"t\"]].to_numpy()), axis=1)\n",
    "\n",
    "y = data[data.index % TRAJECTORY_LENGTH != 0][[\"x_1\", \"y_1\", \"x_2\", \"y_2\", \"x_3\", \"y_3\"]].to_numpy()\n",
    "\n",
    "X_train, y_train = X[train_indices], y[train_indices]\n",
    "X_train_remove = np.where(~X_train[:, :-1].any(axis=1))[0]\n",
    "y_train_remove = np.where(~y_train.any(axis=1))[0]\n",
    "train_remove = np.concatenate((X_train_remove, y_train_remove))\n",
    "X_train, y_train = np.delete(X_train, train_remove, axis=0), np.delete(y_train, train_remove, axis=0)\n",
    "X_val, y_val = X[validation_indices], y[validation_indices]\n",
    "X_val_remove = np.where(~X_val[:, :-1].any(axis=1))[0]\n",
    "y_val_remove = np.where(~y_val.any(axis=1))[0]\n",
    "val_remove = np.concatenate((X_val_remove, y_val_remove))\n",
    "X_val, y_val = np.delete(X_val, val_remove, axis=0), np.delete(y_val, val_remove, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3156123818381802"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_baseline = Pipeline([(\"scaler\", StandardScaler()), (\"lr\", LinearRegression())])\n",
    "pipeline_baseline.fit(X_train, y_train)\n",
    "y_pred = pipeline_baseline.predict(X_val)\n",
    "rms = root_mean_squared_error(y_pred, y_val)\n",
    "rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_submission(pipeline, out_filename):\n",
    "    data_test = pd.read_csv(\"X_test.csv\")\n",
    "    columns = data_test.columns.tolist()\n",
    "    id_column = data_test[columns[0]]\n",
    "    X_test_columns = columns[2:] + [columns[1]]\n",
    "    X_test = data_test[X_test_columns]\n",
    "\n",
    "    y_test = pipeline.predict(X_test)\n",
    "    y_test_df = pd.DataFrame(y_test, columns=[\"x_1\", \"y_1\", \"x_2\", \"y_2\", \"x_3\", \"y_3\"])\n",
    "    y_test_df.insert(0, \"Id\", id_column)\n",
    "    y_test_df.to_csv(out_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bruno Catita\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prepare_submission(pipeline_baseline, \"baseline-model.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree 1:\t1.3156620397000758\n",
      "Degree 2:\t1.2797493522311183\n",
      "Degree 3:\t1.2396277227105201\n",
      "Degree 4:\t1.1983344105136073\n",
      "Degree 1:\t1.315745705734393\n",
      "Degree 2:\t1.2804139633966338\n",
      "Degree 3:\t1.2409536341127765\n",
      "Degree 4:\t1.1974383421807977\n",
      "Degree 1:\t1.315663001254351\n",
      "Degree 2:\t1.2796200171114434\n",
      "Degree 3:\t1.239410845102844\n",
      "Degree 4:\t1.2082505645988686\n",
      "Degree 1:\t1.3156513665304541\n",
      "Degree 2:\t1.2805047257427924\n",
      "Degree 3:\t1.24242588270372\n",
      "Degree 4:\t1.2036115116951076\n",
      "Degree 1:\t1.3158078480921622\n",
      "Degree 2:\t1.2801106608133876\n",
      "Degree 3:\t1.239416615665233\n",
      "Degree 4:\t1.1983902416890981\n",
      "Degree 1:\t1.3158190070082123\n",
      "Degree 2:\t1.2808869972774277\n",
      "Degree 3:\t1.2485283230808177\n",
      "Degree 4:\t1.20214364727661\n",
      "Degree 1:\t1.3158246693670346\n",
      "Degree 2:\t1.2808922978504744\n",
      "Degree 3:\t1.2412462188614117\n",
      "Degree 4:\t1.2032635149574524\n",
      "Degree 1:\t1.315596353435877\n",
      "Degree 2:\t1.2801407384578927\n",
      "Degree 3:\t1.2397331994357053\n",
      "Degree 4:\t1.1974839616765305\n",
      "Degree 1:\t1.3157213140295294\n",
      "Degree 2:\t1.2807503933709146\n",
      "Degree 3:\t1.25665507526147\n",
      "Degree 4:\t1.208943718076375\n",
      "Degree 1:\t1.3157841891118023\n",
      "Degree 2:\t1.2803243705395075\n",
      "Degree 3:\t1.2413812767150927\n",
      "Degree 4:\t1.2718387518462493\n"
     ]
    }
   ],
   "source": [
    "def validate_poly_regression(X_train, y_train, X_val, y_val, regressor=None, degrees=range(1, 15), max_features=None):\n",
    "    best_rms = np.inf\n",
    "    best_model = None\n",
    "    best_degree = 1\n",
    "    for d in degrees:\n",
    "        pipeline = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"poly\", PolynomialFeatures(degree=d)),\n",
    "            (\"regressor\", regressor)\n",
    "        ])\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        rms = root_mean_squared_error(y_pred, y_val)\n",
    "        print(f\"Degree {d}:\\t{rms}\")\n",
    "        if rms < best_rms:\n",
    "            best_rms = rms\n",
    "            best_model = pipeline\n",
    "            best_degree = d\n",
    "    return best_model, best_rms, best_degree\n",
    "\n",
    "# fraction of the training data used for validating polynomial regression\n",
    "TRAIN_SUBSET = 0.05\n",
    "\n",
    "n_examples_poly = int(X_train.shape[0] * TRAIN_SUBSET)\n",
    "degrees = []\n",
    "for _ in range(10):\n",
    "    examples_poly = np.random.choice(X_train.shape[0], n_examples_poly, replace=False)\n",
    "    X_poly, y_poly = X_train[examples_poly], y_train[examples_poly]\n",
    "    _, _, degree = validate_poly_regression(X_poly, y_poly, X_val, y_val, regressor=LinearRegression(), degrees=range(1, 5))\n",
    "    degrees.append(degree)\n",
    "\n",
    "# NOTAS:\n",
    "# - Vê o valor de `validate_poly_regression.n_output_features_`.\n",
    "#   Para o relatório, arranja uma expressão que dá o número de features para um dado grau\n",
    "#   do polinómio e um número de features iniciais.\n",
    "# - Escolhe o melhor grau de PolynomialFeatures, e testa numa célula abaixo um pipeline com vários valores de RidgeCV.\n",
    "#   O \"melhor\" grau não é necessariamente o com RMS mais baixo, take computation into account.\n",
    "# - Faz um histograma com os valores em `degrees`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2398491225971104\n"
     ]
    }
   ],
   "source": [
    "# select this from the experiments above\n",
    "poly_degree = 3\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"poly\", PolynomialFeatures(degree=poly_degree)),\n",
    "    (\"regressor\", RidgeCV(alphas=[0.1, 1.0, 10.0], scoring=\"neg_root_mean_squared_error\"))\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_val)\n",
    "rms = root_mean_squared_error(y_pred, y_val)\n",
    "print(rms)\n",
    "# NOTAS:\n",
    "# - Experimenta com o poly_degree o mais alto possível (máx. 5).\n",
    "# - Usar Ridge parece não fazer diferença nenhuma. Isto pode ser devido a estarmos a usar polinómios de relativamente\n",
    "#   baixo grau (isto pode ser útil no report). Talvez com o poly_degree = 5 faça diferença."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best alpha:\t10.0\n",
      "best score:\t-1.3421121858473835\n"
     ]
    }
   ],
   "source": [
    "ridge_cv = pipeline.named_steps['regressor']\n",
    "print(f\"best alpha:\\t{ridge_cv.alpha_}\")\n",
    "print(f\"best score:\\t{ridge_cv.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bruno Catita\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# having determined that ridge is useless, our best polynomial submission will just be 7\n",
    "pipeline_poly = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"poly\", PolynomialFeatures(degree=3)),\n",
    "    (\"regressor\", LinearRegression())\n",
    "])\n",
    "pipeline_poly.fit(X_train, y_train)\n",
    "prepare_submission(pipeline_poly, \"polynomial_submission.csv\")\n",
    "y_pred = pipeline_poly.predict(X_val)\n",
    "rms = root_mean_squared_error(y_pred, y_val)\n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering my fucking limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new features (distances between each of the three bodies)\n",
    "def add_features(X):\n",
    "    # pairwise distances\n",
    "    d_1_2 = np.linalg.norm(X[:, 0:2] - X[:, 2:4], axis=1).reshape(-1, 1)\n",
    "    d_1_3 = np.linalg.norm(X[:, 0:2] - X[:, 4:6], axis=1).reshape(-1, 1)\n",
    "    d_2_3 = np.linalg.norm(X[:, 2:4] - X[:, 4:6], axis=1).reshape(-1, 1)\n",
    "    # pairwise angles\n",
    "    a_1_2 = np.arctan((X[:, 3] - X[:, 1]) / (X[:, 2] - X[:, 0])).reshape(-1, 1)\n",
    "    a_1_3 = np.arctan((X[:, 5] - X[:, 1]) / (X[:, 4] - X[:, 0])).reshape(-1, 1)\n",
    "    a_2_3 = np.arctan((X[:, 5] - X[:, 3]) / (X[:, 4] - X[:, 1])).reshape(-1, 1)\n",
    "    # center of mass\n",
    "    cm = X[:, 0:2] + X[:, 2:4] + X[:, 4:6]\n",
    "\n",
    "    return np.hstack([X, d_1_2, d_1_3, d_2_3, a_1_2, a_1_3, a_2_3, cm])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"distance_features\", FunctionTransformer(add_features)),\n",
    "    (\"poly\", PolynomialFeatures(degree=2)),\n",
    "    (\"regressor\", LinearRegression())\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_val)\n",
    "rms = root_mean_squared_error(y_pred, y_val)\n",
    "print(rms)\n",
    "# no feats -> 1.316\n",
    "# only distances -> 1.301\n",
    "# distances + angles -> 1.290\n",
    "# distances + angles + cm -> 1.290\n",
    "# P2 + only distances -> 1.252\n",
    "# P2 + distances + angles -> 1.268\n",
    "# P2 + distances + angles + cm -> 1.205 (13.8s)\n",
    "# P2 + distances + cm -> 1.257\n",
    "#\n",
    "#\n",
    "# FINAL: maybe do P3/4 + distances + angles + cm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Nipples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_knn_regression(X_train, y_train, X_val, y_val, nns=range(30,40)):\n",
    "    results = {}\n",
    "    for k in nns:\n",
    "        pipeline = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"regressor\", KNeighborsRegressor(k))\n",
    "        ])\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        rms = root_mean_squared_error(y_pred, y_val)\n",
    "        results[k] = rms\n",
    "    return results\n",
    "        \n",
    "# fraction of the training data used for validating KNN neighbour choice\n",
    "KNN_SUBSET = 1\n",
    "\n",
    "n_examples_knn = int(X_train.shape[0] * KNN_SUBSET)\n",
    "examples_knn = np.random.choice(X_train.shape[0], n_examples_knn, replace=False)\n",
    "X_knn, y_knn = X_train[examples_knn], y_train[examples_knn]\n",
    "results = validate_knn_regression(X_knn, y_knn, X_val, y_val)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# having determined that ridge is useless, our best polynomial submission will just be 7\n",
    "pipeline_knn = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"poly\", PolynomialFeatures(degree=3)),\n",
    "    (\"regressor\", KNeighborsRegressor(32))\n",
    "])\n",
    "pipeline_knn.fit(X_train, y_train)\n",
    "prepare_submission(pipeline_knn, \"knn_submission.csv\")\n",
    "y_pred = pipeline_knn.predict(X_val)\n",
    "rms = root_mean_squared_error(y_pred, y_val)\n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
