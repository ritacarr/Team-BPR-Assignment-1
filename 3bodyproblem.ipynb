{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "N_TRAJECTORIES = 5000\n",
    "TRAJECTORY_LENGTH = 257\n",
    "VALIDATION_SPLIT = 0.2  # fraction of the training data used for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1285000 / 257 = 5000 trajectories\n",
    "data = pd.read_csv(\"X_train.csv\")\n",
    "n_lines = data.shape[0]\n",
    "assert n_lines / TRAJECTORY_LENGTH == N_TRAJECTORIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#array([0, 1, ..., 4999]) with IDs of all trajectories\n",
    "all_trajectories = np.arange(N_TRAJECTORIES)\n",
    "#number of trajectories (e.g., 1000) used for validation\n",
    "n_trajectories_val = int(N_TRAJECTORIES * 0.2)\n",
    "#indices (from 0 to 4999) that identify the validation trajectories\n",
    "trajectories_val = np.random.choice(N_TRAJECTORIES, n_trajectories_val, replace=False)\n",
    "#the train trajectories IDs are those remaining from removing the validation trajectories from all 5000\n",
    "trajectories_train = np.setdiff1d(all_trajectories, trajectories_val)\n",
    "n_trajectories_train = N_TRAJECTORIES - n_trajectories_val\n",
    "\n",
    "#now we find the indices of the rows corresponding to each set (train and validation) of trajectories\n",
    "validation_indices = trajectories_val.repeat(TRAJECTORY_LENGTH - 1) * (TRAJECTORY_LENGTH - 1)\n",
    "validation_indices += np.tile(np.arange(TRAJECTORY_LENGTH - 1), n_trajectories_val)\n",
    "train_indices = trajectories_train.repeat(TRAJECTORY_LENGTH - 1) * (TRAJECTORY_LENGTH - 1)\n",
    "train_indices += np.tile(np.arange(TRAJECTORY_LENGTH - 1), n_trajectories_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[all_trajectories * TRAJECTORY_LENGTH][[\"x_1\", \"y_1\", \"x_2\", \"y_2\", \"x_3\", \"y_3\"]]\n",
    "X = X.to_numpy().repeat(TRAJECTORY_LENGTH - 1, axis=0)\n",
    "X = np.concatenate((X, data[data.index % TRAJECTORY_LENGTH != 0][[\"t\"]].to_numpy()), axis=1)\n",
    "\n",
    "y = data[data.index % TRAJECTORY_LENGTH != 0][[\"x_1\", \"y_1\", \"x_2\", \"y_2\", \"x_3\", \"y_3\"]].to_numpy()\n",
    "\n",
    "X_train, y_train = X[train_indices], y[train_indices]\n",
    "X_train_remove = np.where(~X_train[:, :-1].any(axis=1))[0]\n",
    "y_train_remove = np.where(~y_train.any(axis=1))[0]\n",
    "train_remove = np.concatenate((X_train_remove, y_train_remove))\n",
    "X_train, y_train = np.delete(X_train, train_remove, axis=0), np.delete(y_train, train_remove, axis=0)\n",
    "X_val, y_val = X[validation_indices], y[validation_indices]\n",
    "X_val_remove = np.where(~X_val[:, :-1].any(axis=1))[0]\n",
    "y_val_remove = np.where(~y_val.any(axis=1))[0]\n",
    "val_remove = np.concatenate((X_val_remove, y_val_remove))\n",
    "X_val, y_val = np.delete(X_val, val_remove, axis=0), np.delete(y_val, val_remove, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.315613931935684"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_baseline = Pipeline([(\"scaler\", StandardScaler()), (\"lr\", LinearRegression())])\n",
    "pipeline_baseline.fit(X_train, y_train)\n",
    "y_pred = pipeline_baseline.predict(X_val)\n",
    "rms = root_mean_squared_error(y_pred, y_val)\n",
    "rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test set\n",
    "def prepare_submission(pipeline, out_filename):\n",
    "    data_test = pd.read_csv(\"X_test.csv\")\n",
    "    columns = data_test.columns.tolist()\n",
    "    id_column = data_test[columns[0]]\n",
    "    X_test_columns = columns[2:] + [columns[1]]\n",
    "    X_test = data_test[X_test_columns]\n",
    "\n",
    "    y_test = pipeline.predict(X_test)\n",
    "    y_test_df = pd.DataFrame(y_test, columns=[\"x_1\", \"y_1\", \"x_2\", \"y_2\", \"x_3\", \"y_3\"])\n",
    "    y_test_df.insert(0, \"Id\", id_column)\n",
    "    y_test_df.to_csv(out_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming y_val is your data with 6 columns (replace this with your actual data)\n",
    "df = pd.DataFrame(y_val, columns=[\"x_1\", \"y_1\", \"x_2\", \"y_2\", \"x_3\", \"y_3\"])\n",
    "\n",
    "# Create a pairplot (taking a sample of 200 rows for efficiency)\n",
    "pairplot = sns.pairplot(df.sample(200), kind=\"hist\")\n",
    "\n",
    "# Save the pairplot as an image\n",
    "pairplot.savefig('pairplot_image.png', format='png', dpi=300)  # Save as PNG with high resolution\n",
    "\n",
    "# Show the plot (optional, if you want to display it as well)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_y_yhat(y_val,y_pred, plot_title = \"plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_dict = {\n",
    "    'x_1': residuals[:, 0],\n",
    "    'y_1': residuals[:, 1],\n",
    "    'x_2': residuals[:, 2],\n",
    "    'y_2': residuals[:, 3],\n",
    "    'x_3': residuals[:, 4],\n",
    "    'y_3': residuals[:, 5]\n",
    "}\n",
    "\n",
    "# Create a density plot for each residual (x_1, y_1, x_2, y_2, x_3, y_3)\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, (label, residual) in enumerate(residuals_dict.items()):\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    sns.kdeplot(residual, fill=True, color='blue')\n",
    "    plt.title(f\"Residual Density Plot for {label}\")\n",
    "    plt.xlabel(\"Residuals\")\n",
    "    plt.ylabel(\"Density\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_submission(pipeline_baseline, \"baseline-model.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree 1:\t1.315661963520277\n",
      "Degree 2:\t1.2797359713150513\n",
      "Degree 3:\t1.2396127925990426\n",
      "Degree 4:\t1.1973448374899642\n",
      "Degree 5:\t1.1600737247120254\n",
      "Degree 1:\t1.315745769609998\n",
      "Degree 2:\t1.2803953969413397\n",
      "Degree 3:\t1.2517200751204756\n",
      "Degree 4:\t1.1976992916077231\n",
      "Degree 5:\t1.1588309031281439\n",
      "Degree 1:\t1.3156630991501936\n",
      "Degree 2:\t1.2795984328502552\n",
      "Degree 3:\t1.2394853888381165\n",
      "Degree 4:\t1.1978607545310953\n",
      "Degree 5:\t1.1607230508596411\n",
      "Degree 1:\t1.3156513201476983\n",
      "Degree 2:\t1.2803656865978421\n",
      "Degree 3:\t1.2405431111314071\n",
      "Degree 4:\t1.1992263307651532\n",
      "Degree 5:\t1.1609827529538441\n",
      "Degree 1:\t1.3158077518290952\n",
      "Degree 2:\t1.2800804150320353\n",
      "Degree 3:\t1.240582932072795\n",
      "Degree 4:\t1.1984752733993005\n",
      "Degree 5:\t1.1597857447840225\n",
      "Degree 1:\t1.315813644999093\n",
      "Degree 2:\t1.2806978297100966\n",
      "Degree 3:\t1.2416043785548516\n",
      "Degree 4:\t1.2015242106474917\n",
      "Degree 5:\t1.1640306387463453\n",
      "Degree 1:\t1.3158248239855592\n",
      "Degree 2:\t1.2808579122592583\n",
      "Degree 3:\t1.241085720453982\n",
      "Degree 4:\t1.1990294740414533\n",
      "Degree 5:\t1.1752075198815883\n",
      "Degree 1:\t1.3155964223716312\n",
      "Degree 2:\t1.2800741267191462\n",
      "Degree 3:\t1.2421370674344459\n",
      "Degree 4:\t1.1966289148280225\n",
      "Degree 5:\t1.1574662915305722\n",
      "Degree 1:\t1.3157216359764938\n",
      "Degree 2:\t1.281002062788936\n",
      "Degree 3:\t1.240203215465282\n",
      "Degree 4:\t1.200229248429667\n",
      "Degree 5:\t1.1624141915903738\n",
      "Degree 1:\t1.3157842964287234\n",
      "Degree 2:\t1.2802846209694663\n",
      "Degree 3:\t1.2410066802213457\n",
      "Degree 4:\t1.200413411900723\n",
      "Degree 5:\t1.1618512571990134\n"
     ]
    }
   ],
   "source": [
    "def validate_poly_regression(X_train, y_train, X_val, y_val, regressor=None, degrees=range(1, 15), max_features=None):\n",
    "    best_rms = np.inf\n",
    "    best_model = None\n",
    "    best_degree = 1\n",
    "    for d in degrees:\n",
    "        pipeline = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"poly\", PolynomialFeatures(degree=d)),\n",
    "            (\"regressor\", regressor)\n",
    "        ])\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        rms = root_mean_squared_error(y_pred, y_val)\n",
    "        print(f\"Degree {d}:\\t{rms}\")\n",
    "        if rms < best_rms:\n",
    "            best_rms = rms\n",
    "            best_model = pipeline\n",
    "            best_degree = d\n",
    "    return best_model, best_rms, best_degree\n",
    "\n",
    "# fraction of the training data used for validating polynomial regression\n",
    "TRAIN_SUBSET = 0.05\n",
    "\n",
    "#10 random train subsets and rmse for each polynomial degree\n",
    "n_examples_poly = int(X_train.shape[0] * TRAIN_SUBSET)\n",
    "degrees = []\n",
    "for _ in range(10):\n",
    "    examples_poly = np.random.choice(X_train.shape[0], n_examples_poly, replace=False)\n",
    "    X_poly, y_poly = X_train[examples_poly], y_train[examples_poly]\n",
    "    _, _, degree = validate_poly_regression(X_poly, y_poly, X_val, y_val, regressor=LinearRegression(), degrees=range(1, 6))\n",
    "    degrees.append(degree)\n",
    "\n",
    "# - Escolhe o melhor grau de PolynomialFeatures, e testa numa célula abaixo um pipeline com vários valores de RidgeCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_y_yhat(y_val,y_pred, plot_title = \"plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_dict = {\n",
    "    'x_1': residuals[:, 0],\n",
    "    'y_1': residuals[:, 1],\n",
    "    'x_2': residuals[:, 2],\n",
    "    'y_2': residuals[:, 3],\n",
    "    'x_3': residuals[:, 4],\n",
    "    'y_3': residuals[:, 5]\n",
    "}\n",
    "\n",
    "# Create a density plot for each residual (x_1, y_1, x_2, y_2, x_3, y_3)\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, (label, residual) in enumerate(residuals_dict.items()):\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    sns.kdeplot(residual, fill=True, color='blue')\n",
    "    plt.title(f\"Residual Density Plot for {label}\")\n",
    "    plt.xlabel(\"Residuals\")\n",
    "    plt.ylabel(\"Density\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "330\n",
      "792\n"
     ]
    }
   ],
   "source": [
    "#number of features according to polynomial degree\n",
    "def num_polynomial_features(n_features, degree):\n",
    "    num_features = 0\n",
    "    for d in range(degree + 1):\n",
    "        num_features += comb(n_features + d - 1, d)\n",
    "    return num_features\n",
    "\n",
    "n_features = 7 #number of columns in X_train\n",
    "degree = 3\n",
    "print(num_polynomial_features(n_features, degree))  #120 features\n",
    "degree = 4\n",
    "print(num_polynomial_features(n_features, degree))  #330 features\n",
    "degree = 5\n",
    "print(num_polynomial_features(n_features, degree))  #792 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2398491225107946\n"
     ]
    }
   ],
   "source": [
    "#RidgeCV: select this from the experiments above\n",
    "#done for 3/5 degree\n",
    "poly_degree = 3\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"poly\", PolynomialFeatures(degree=poly_degree)),\n",
    "    (\"regressor\", RidgeCV(alphas=[0.1, 1.0, 10.0], scoring=\"neg_root_mean_squared_error\"))\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_val)\n",
    "rms = root_mean_squared_error(y_pred, y_val)\n",
    "print(rms)\n",
    "#Ridge doesn't seem to make difference, could be explained due to a low degree polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best alpha:\t10.0\n",
      "best score:\t-1.3421121858473832\n"
     ]
    }
   ],
   "source": [
    "ridge_cv = pipeline.named_steps['regressor']\n",
    "print(f\"best alpha:\\t{ridge_cv.alpha_}\")\n",
    "print(f\"best score:\\t{ridge_cv.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#polynomial regression chosen: 3 and 5\n",
    "#7 too much for the computer to handle\n",
    "pipeline_poly = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"poly\", PolynomialFeatures(degree=5)),\n",
    "    (\"regressor\", LinearRegression())\n",
    "])\n",
    "pipeline_poly.fit(X_train, y_train)\n",
    "prepare_submission(pipeline_poly, \"polynomial_submission.csv\")\n",
    "y_pred = pipeline_poly.predict(X_val)\n",
    "rms = root_mean_squared_error(y_pred, y_val)\n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_y_yhat(y_val,y_pred, plot_title = \"plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_dict = {\n",
    "    'x_1': residuals[:, 0],\n",
    "    'y_1': residuals[:, 1],\n",
    "    'x_2': residuals[:, 2],\n",
    "    'y_2': residuals[:, 3],\n",
    "    'x_3': residuals[:, 4],\n",
    "    'y_3': residuals[:, 5]\n",
    "}\n",
    "\n",
    "# Create a density plot for each residual (x_1, y_1, x_2, y_2, x_3, y_3)\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, (label, residual) in enumerate(residuals_dict.items()):\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    sns.kdeplot(residual, fill=True, color='blue')\n",
    "    plt.title(f\"Residual Density Plot for {label}\")\n",
    "    plt.xlabel(\"Residuals\")\n",
    "    plt.ylabel(\"Density\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1996066255538815\n"
     ]
    }
   ],
   "source": [
    "# create new features (distances between each of the three bodies)\n",
    "def add_features(X):\n",
    "    # pairwise distances\n",
    "    d_1_2 = np.linalg.norm(X[:, 0:2] - X[:, 2:4], axis=1).reshape(-1, 1)\n",
    "    d_1_3 = np.linalg.norm(X[:, 0:2] - X[:, 4:6], axis=1).reshape(-1, 1)\n",
    "    d_2_3 = np.linalg.norm(X[:, 2:4] - X[:, 4:6], axis=1).reshape(-1, 1)\n",
    "    # pairwise angles\n",
    "    a_1_2 = np.arctan((X[:, 3] - X[:, 1]) / (X[:, 2] - X[:, 0])).reshape(-1, 1)\n",
    "    a_1_3 = np.arctan((X[:, 5] - X[:, 1]) / (X[:, 4] - X[:, 0])).reshape(-1, 1)\n",
    "    a_2_3 = np.arctan((X[:, 5] - X[:, 3]) / (X[:, 4] - X[:, 0])).reshape(-1, 1)\n",
    "    # center of mass\n",
    "    cm = X[:, 0:2] + X[:, 2:4] + X[:, 4:6]\n",
    "\n",
    "    return np.hstack([X, d_1_2, d_1_3, d_2_3, a_1_2, a_1_3, a_2_3, cm])\n",
    "\n",
    "def remove_features(X):\n",
    "    return np.delete(X, [0, 2, 4], axis=1)\n",
    "\n",
    "pipeline_eng = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"distance_features\", FunctionTransformer(add_features)),\n",
    "    (\"poly\", PolynomialFeatures(degree=2)),\n",
    "    (\"regressor\", LinearRegression())\n",
    "])\n",
    "\n",
    "#pipeline = Pipeline([\n",
    "#    (\"scaler\", StandardScaler()),\n",
    "    #(\"distance_features\", FunctionTransformer(add_features)),\n",
    "#    (\"remove_features\", FunctionTransformer(remove_features)),\n",
    "#    (\"regressor\", LinearRegression())\n",
    "#])\n",
    "\n",
    "pipeline_eng.fit(X_train, y_train)\n",
    "y_pred = pipeline_eng.predict(X_val)\n",
    "rms = root_mean_squared_error(y_pred, y_val)\n",
    "print(rms)\n",
    "# no feats -> 1.316\n",
    "# only distances -> 1.301\n",
    "# distances + angles -> 1.290\n",
    "# distances + angles + cm -> 1.290\n",
    "# P2 + only distances -> 1.252\n",
    "# P2 + distances + angles -> 1.268\n",
    "# P2 + distances + angles + cm -> 1.205 (13.8s)\n",
    "# P2 + distances + cm -> 1.257\n",
    "#\n",
    "#\n",
    "# FINAL: maybe do P3/4 + distances + angles + cm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_y_yhat(y_val,y_pred, plot_title = \"plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_dict = {\n",
    "    'x_1': residuals[:, 0],\n",
    "    'y_1': residuals[:, 1],\n",
    "    'x_2': residuals[:, 2],\n",
    "    'y_2': residuals[:, 3],\n",
    "    'x_3': residuals[:, 4],\n",
    "    'y_3': residuals[:, 5]\n",
    "}\n",
    "\n",
    "# Create a density plot for each residual (x_1, y_1, x_2, y_2, x_3, y_3)\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, (label, residual) in enumerate(residuals_dict.items()):\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    sns.kdeplot(residual, fill=True, color='blue')\n",
    "    plt.title(f\"Residual Density Plot for {label}\")\n",
    "    plt.xlabel(\"Residuals\")\n",
    "    plt.ylabel(\"Density\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_submission(pipeline_eng, \"augmented_polynomial_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#prepare_submission(pipeline_eng, \"reduced_polynomial_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{30: 0.8229688612240088,\n",
       " 31: 0.821571217151439,\n",
       " 32: 0.8208578818724056,\n",
       " 33: 0.8204982220856754,\n",
       " 34: 0.8195408748460857,\n",
       " 35: 0.8207365815065937,\n",
       " 36: 0.821180270165275,\n",
       " 37: 0.820257923304951,\n",
       " 38: 0.8197082170719893,\n",
       " 39: 0.8214370915791646}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_knn_regression(X_train, y_train, X_val, y_val, nns=range(30,40)):\n",
    "    results = {}\n",
    "    for k in nns:\n",
    "        pipeline = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"regressor\", KNeighborsRegressor(k))\n",
    "        ])\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        rms = root_mean_squared_error(y_pred, y_val)\n",
    "        results[k] = rms\n",
    "    return results\n",
    "        \n",
    "# fraction of the training data used for validating KNN neighbour choice\n",
    "KNN_SUBSET = 1\n",
    "\n",
    "n_examples_knn = int(X_train.shape[0] * KNN_SUBSET)\n",
    "examples_knn = np.random.choice(X_train.shape[0], n_examples_knn, replace=False)\n",
    "X_knn, y_knn = X_train[examples_knn], y_train[examples_knn]\n",
    "results = validate_knn_regression(X_knn, y_knn, X_val, y_val)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k=10, should've tested for more k numbers\n",
    "pipeline_knn = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"distance_features\", FunctionTransformer(add_features)),\n",
    "    (\"poly\", PolynomialFeatures(degree=2)),\n",
    "    (\"regressor\", KNeighborsRegressor(10))\n",
    "])\n",
    "pipeline_knn.fit(X_train, y_train)\n",
    "print(\"Fit!\")\n",
    "prepare_submission(pipeline_knn, \"knn_submission.csv\")\n",
    "y_pred = pipeline_knn.predict(X_val)\n",
    "rms = root_mean_squared_error(y_pred, y_val)\n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_y_yhat(y_val,y_pred, plot_title = \"plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_dict = {\n",
    "    'x_1': residuals[:, 0],\n",
    "    'y_1': residuals[:, 1],\n",
    "    'x_2': residuals[:, 2],\n",
    "    'y_2': residuals[:, 3],\n",
    "    'x_3': residuals[:, 4],\n",
    "    'y_3': residuals[:, 5]\n",
    "}\n",
    "\n",
    "# Create a density plot for each residual (x_1, y_1, x_2, y_2, x_3, y_3)\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, (label, residual) in enumerate(residuals_dict.items()):\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    sns.kdeplot(residual, fill=True, color='blue')\n",
    "    plt.title(f\"Residual Density Plot for {label}\")\n",
    "    plt.xlabel(\"Residuals\")\n",
    "    plt.ylabel(\"Density\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
